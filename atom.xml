<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[My Octopress Blog]]></title>
  <link href="http://TakashiNakagawa.github.io/atom.xml" rel="self"/>
  <link href="http://TakashiNakagawa.github.io/"/>
  <updated>2013-06-10T21:46:35+09:00</updated>
  <id>http://TakashiNakagawa.github.io/</id>
  <author>
    <name><![CDATA[Your Name]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[sicp 問1.23]]></title>
    <link href="http://TakashiNakagawa.github.io/blog/2013/06/10/sicp2/"/>
    <updated>2013-06-10T20:41:00+09:00</updated>
    <id>http://TakashiNakagawa.github.io/blog/2013/06/10/sicp2</id>
    <content type="html"><![CDATA[<h2>問1.23</h2>

<blockquote><p>本節の始めのsmalleset-divisorは多くの不要な計算をする：数が２で割切れるか調べた後は、より大きい偶数で割けれるか調べるのは無意味である。<br/>
test-divisorが使う値は、2,3,4,5,…であるべきだ。<br/>
この変更を実装するため、入力が２なら３を返し、そうでなければ入力に２足したものを返す手続きnextを定義せよ。<br/>
smallest-divisorを(+ test-divisor 1)ではなく、(next test-divisor)を使うように修正せよ。<br/>
このように修正したsmallest-divisorにしたtimed-prime-testで、問題1.22で見つけた12個の素数をテストせよ。<br/>
この修正はテストのステップを半分にしたので、２倍速く走る事が期待される。この期待は確かめられるか。<br/>
そうでなければ、得られた２つのアルゴリズムの速度の比はどうであったか。それが２と違う事情を説明せよ。</p></blockquote>

<h3>解答</h3>

<p>nextの実装はこんな感じ</p>

<pre><code>(define (next input)
  (cond
    ((= input 2) 3)
    (else (+ input 2))))
</code></pre>

<p><img src="http://TakashiNakagawa.github.io/images/blog/graph.png" alt="alt" /></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[opencv]]></title>
    <link href="http://TakashiNakagawa.github.io/blog/2013/05/29/opencv/"/>
    <updated>2013-05-29T10:39:00+09:00</updated>
    <id>http://TakashiNakagawa.github.io/blog/2013/05/29/opencv</id>
    <content type="html"><![CDATA[<h1>opencvの特徴点対応付け</h1>

<p><a href="http://miyabiarts.wordpress.com/2011/01/21/opencv%E3%82%92%E7%94%A8%E3%81%84%E3%81%9F%E7%89%B9%E5%BE%B4%E7%82%B9%E5%AF%BE%E5%BF%9C%E4%BB%98%E3%81%91/">ここのページ</a>
を参考にさせて頂いた</p>

<pre><code>#include &lt;iostream&gt;
#include &lt;opencv2/highgui/highgui.hpp&gt;
#include &lt;opencv2/nonfree/nonfree.hpp&gt;
#include &lt;opencv2/legacy/legacy.hpp&gt;    

int main(int argc, char *argv[])
{
    //画像読み込み
    cv::Mat colorImg1 = cv::imread("lady1.jpeg");
    cv::Mat colorImg2 = cv::imread("lady2.jpeg");
    if(colorImg1.empty() || colorImg2.empty()){
        std::cout &lt;&lt; "No Image" &lt;&lt; std::endl;
        return -1;
    }

    //特徴点抽出用のグレー画像用意
    cv::Mat grayImg1, grayImg2;
    cv::cvtColor(colorImg1, grayImg1, CV_BGR2GRAY);
    cv::normalize(grayImg1, grayImg1, 0, 255, cv::NORM_MINMAX);
    cv::cvtColor(colorImg2, grayImg2, CV_BGR2GRAY);
    cv::normalize(grayImg2, grayImg2, 0, 255, cv::NORM_MINMAX);

    //SIFT
    //    cv::SiftFeatureDetector detector;
    //    cv::SiftDescriptorExtractor extractor;
    //SURF
    cv::SurfFeatureDetector detector(1000);
    cv::SurfDescriptorExtractor extractor;

    //画像から特徴点を検出
    std::vector&lt;cv::KeyPoint&gt; keypoints1;
    detector.detect(grayImg1, keypoints1);
    std::vector&lt;cv::KeyPoint&gt; keypoints2;
    detector.detect(grayImg2, keypoints2);

    //画像の特徴点における特徴量を抽出
    cv::Mat descriptors1;
    extractor.compute(grayImg1, keypoints1, descriptors1);
    cv::Mat descriptors2;
    extractor.compute(grayImg2, keypoints2, descriptors2);

    //特徴点の対応付け
    std::vector&lt;cv::DMatch&gt; matches;
    cv::BruteForceMatcher&lt;cv::L2&lt;float&gt; &gt; matcher;
    matcher.match(descriptors1, descriptors2, matches);

    //ソートしたn番目までの対応線を表示させる。nth_elementは要素を基準要素よりも手前に移動させるある種のソート
    int N=50;
    nth_element(matches.begin(), matches.begin()+N-1, matches.end());
    matches.erase(matches.begin()+N, matches.end());

    //対応づけされた画像の用意
    cv::Mat matchedImg;
    cv::drawMatches(colorImg1, keypoints1, colorImg2, keypoints2, matches, matchedImg);

    /// 画像を表示するウィンドウの名前，プロパティ
    // CV_WINDOW_AUTOSIZE : ウィンドウサイズを画像サイズに合わせる
    // CV_WINDOW_FREERATIO : ウィンドウのアスペクト比を固定しない
    cv::namedWindow("image", CV_WINDOW_AUTOSIZE|CV_WINDOW_FREERATIO);
    // ウィンドウ名でウィンドウを指定して，そこに画像を描画
    cv::imshow("image", matchedImg);

    // キー入力を（無限に）待つ
    cv::waitKey(0);
    return 0;
}
</code></pre>

<p>画像の読み込み時にフルパスでないとなぜか読み込めなかった。<br/>
とりあえずworking directoryを変更する事で対応<br/>
Edit Schemeから変更する（<a href="http://d.hatena.ne.jp/nakamura001/20120729/1343520758">こちらのサイトを参考</a>)</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Octopressのテーマ設定]]></title>
    <link href="http://TakashiNakagawa.github.io/blog/2013/05/25/theme/"/>
    <updated>2013-05-25T08:49:00+09:00</updated>
    <id>http://TakashiNakagawa.github.io/blog/2013/05/25/theme</id>
    <content type="html"><![CDATA[<h1>Octopressのテーマの変更方法</h1>

<p><a href="http://www.evolument.com/blog/2013/03/02/top-10-plus-octopress-themes/">こちらのリンク先</a>から選んで変更するだけ</p>

<p>今適用しているテーマに変更する場合は、</p>

<pre><code>cd octopress
git clone git://github.com/sevenadrian/foxslide.git .themes/foxslide
rake install['foxslide']
rake generate
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Mastering OpenCV 8章]]></title>
    <link href="http://TakashiNakagawa.github.io/blog/2013/05/17/opencv8/"/>
    <updated>2013-05-17T17:25:00+09:00</updated>
    <id>http://TakashiNakagawa.github.io/blog/2013/05/17/opencv8</id>
    <content type="html"><![CDATA[<h2><em>Chapter 8: Face Recognition using Eigenfaces or Fisherfaces</em></h2>

<h3>概要</h3>

<p>Mastering Opencv with Practical Computer Vision Projectsの8章を読んでみた。</p>

<p>アプリを実行して動作の確認をする事が出来たので、改めて内容をまとめて直してみる。</p>

<p>本章の内容は以下の４つのステップに分かれている。
各ステップは丁寧に書かれており、最終的なアプリはEigenfaces or Fisherfacesを<br/>
ソースコード上で切り分けて実行する事が出来るようになっている。</p>

<ul>
<li>Step 1: Face detection</li>
<li>Step 2: Face preprocessing</li>
<li>Step 3: Collecting faces and learning from them</li>
<li>Step 4: Face recognition</li>
</ul>


<h2>Step1: Face detection  </h2>

<ul>
<li>Haar-based face detector</li>
<li>LBP-based face detector</li>
</ul>


<h5>Implementing face detection using OpenCV</h5>

<h5>Loading a Haar or LBP detector for object or face detection</h5>

<pre><code>CascadeClassifier faceDetector:
try{
  faceDetector.load(fileCascadeFilenname);
} catch (cv::Exception e){}
if (faceDetector.empty()){
  cerr &lt;&lt; "Error &lt;&lt; endl;
  exit(1);
}
</code></pre>

<h5>Detecting an object using the Haar or LBP Classifier</h5>

<ol>
<li>Grayscale color conversion</li>
<li>Shringking the camera image</li>
<li>Histogram equalization</li>
</ol>


<h5>Detecting the face</h5>

<ul>
<li>CascadeClassifier::detectMultiScale()</li>
</ul>


<h2>Step2: Face preprocessing</h2>

<p>顔認識は、明るさ、顔の向き、顔の表情などに影響を受けやすい。<br/>
histogram equalizationだけで十分な場合もあるが、それでは不十分な場合も多い。</p>

<h5>Eye detection</h5>

<p>false positive : 検出すべきでないものも検出してしまうこと<br/>
false negative : 検出すべきものを検出しないこと</p>

<p>目を検出することにより、false positiveを避ける事が出来る</p>

<p>目を検出するもの</p>

<ul>
<li>haarcascade_mcs_lefteye.xml( or righteye)</li>
<li>haarcascade_lefteye_2splits.xml( or righteye)</li>
</ul>


<p>開いている目のみを検出するもの</p>

<ul>
<li>haarcascade_eye.xml</li>
<li>haarcascade_eye_tree_eyeglasses.xml</li>
</ul>


<h5>Eye search regions</h5>

<p>顔と目の検出が成功したら、以下の４つを実行</p>

<ul>
<li><p>Geometrical transformation and cropping</p>

<ul>
<li>Rotate the face so that the tow eyes are horizontal</li>
<li>Scale the face so that the distance between the tow eyes is always the same</li>
<li>Translate the face so that the eyes are always centered horizontally and at a desired height</li>
<li>Crop the outer parts of the face, since we want to crop away the image background, hair, forehead, ears, and chin</li>
</ul>
</li>
<li><p>Separate histogram equalization for left and right sides</p></li>
<li>Smoothing

<ul>
<li>Bilateral Filter

<ul>
<li>エッジ保持平滑化フィルタ、画像をスムーズにしつつもエッジ部分はぼかす事のない効果を持っている</li>
<li>ガウスぼかしをする際に、画素間の距離で重みを決めるのではなく、輝度の差も見て、変化が大きいところは重みを小さくすることによってエッジを残す</li>
</ul>
</li>
</ul>
</li>
<li>Elliptical mask</li>
</ul>


<h2>Step3: Collecting faces and learing from them</h2>

<h5>Collecting preprocessed faces for training</h5>

<p>集めた2つの画像の一致を調べるには次のようにする。</p>

<pre><code>double getSimilarity(const Mat A, const Mat B){
  double errorL2 = norm(A, B, CV_L2);
  double similarity = errorL2 / (double)(A.rows * A.cols);
  return similarity;
}
</code></pre>

<p>getSimilarityの値が0.3以上あれば画像が動いたと判断出来る</p>

<h5>Traing the face recognition system from collected faces</h5>

<p>顔認識を行うために、適切な機会学習アルゴリズムを用いる必要がある。</p>

<ul>
<li>Eigenfaces( Principal Component Analysis(PCA) )</li>
<li>Fisherfaces( Linear Discriminant Analysis(LDA) )</li>
<li>Other classic face recognition algorithms( <a href="http://www.face-rec.org/algorithms/">http://www.face-rec.org/algorithms/</a> )</li>
<li>Newer face recognition algorithms ( CVPR, ICCVなど )</li>
</ul>


<h5>Viewing the learned knowledge</h5>

<p>EigenfacesとFisherfacesの内部データ構造は同じ。<br/>
両者は一次元の固有ベクトルに基づいている。</p>

<h5>Average face</h5>

<h5>Eigenvalues, Eigenfaces, and Fisherfaces</h5>

<p>Eigenfacesの場合、3人×4つの顔：固有値は3×４＝12<br/>
Fisherfacesの場合、3人×4つの顔: 固有値は2つ ← なぜ3つではない？？</p>

<h2>Step4: Face recognition</h2>

<h5>Face identification: Recognizing people from their face</h5>

<h5>Face verificatoin: Validating that it is the claimed person</h5>

<p>to obtain good recognition accuracy, you will need to ensure
that the training set of each person covers the hull range of lighting conditions,
facial expressions, and angles that you expect to test with.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[今日からOctopress]]></title>
    <link href="http://TakashiNakagawa.github.io/blog/2013/05/17/hello/"/>
    <updated>2013-05-17T10:03:00+09:00</updated>
    <id>http://TakashiNakagawa.github.io/blog/2013/05/17/hello</id>
    <content type="html"><![CDATA[<h1>Octopressを使いはじめる</h1>

<p>以前もちょっとやっていたことがあったけども途中で止めてしまった。<br/>
また再開することに。<br/>
ここには技術的な事を書いて行きたいと思う。</p>

<p>何はともあれOctopressに投稿するための作業をメモしておく</p>

<pre><code>$ rake new_post["test"] # 新規記事を作成  
$ rake preview # preview, localhost:4000にアクセス  
$ rake generate # convert to html  
$ rake deploy # publish  
</code></pre>

<p>次回はOctopressのレイアウトを変更の仕方について調べたい</p>
]]></content>
  </entry>
  
</feed>
